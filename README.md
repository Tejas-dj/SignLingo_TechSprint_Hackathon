# ğŸ¤Ÿ SignLingo: AI-Powered Bidirectional Sign Language Translator

**SignLingo** is a real-time accessibility tool designed to bridge the communication gap between the Deaf/Hard-of-Hearing community and the hearing world. Built with advanced Computer Vision and Generative AI, it provides seamless **Sign-to-Text** and **Text-to-Sign** translation.

---

## ğŸš€ Key Features

### 1. ğŸ“· Real-Time Sign to Text
* **Custom AI Model:** Uses a custom-trained TensorFlow/Keras Neural Network (trained on the Kaggle ASL Alphabet dataset) to recognize hand gestures with 95%+ accuracy.
* **Smart Stabilization:** Implements a `Deque` sliding window algorithm to remove "flickering" and ensure smooth letter detection.
* **Predictive Text:** Auto-complete engine suggests words as you spell (e.g., spelling "P-Y-T" suggests "PYTHON"), significantly speeding up communication.

### 2. ğŸ¤– Text to Sign (The "Infinite Dictionary")
* **Visual Translation:** Converts text input into dynamic Sign Language GIFs.
* **Fingerspelling Engine:** If a word (e.g., "ChatGPT") is not in the GIF database, the system automatically falls back to **Fingerspelling** the word letter-by-letter using a visual asset library.

### 3. ğŸ® Gamified Learning Mode
* **Interactive Tutor:** A "Duolingo-style" module that challenges users to mimic specific ASL signs.
* **Real-time Feedback:** Uses Computer Vision to score the user's accuracy instantly.

---

## ğŸ› ï¸ Tech Stack

* **Frontend:** Streamlit (Python)
* **Computer Vision:** MediaPipe (Hand Landmarks), OpenCV
* **AI/ML:** TensorFlow (Keras), Scikit-Learn
* **Hardware Acceleration:** Optimized for NVIDIA RTX GPUs (tested on RTX 3050).

---

## ğŸ“‚ Project Structure

```text
SignLingo/
â”œâ”€â”€ letters&numbers/    # Database of images for the Fingerspelling Engine
â”œâ”€â”€ venv/               # Python Virtual Environment
â”œâ”€â”€ app.py              # Main application entry point (Streamlit)
â”œâ”€â”€ assets.py           # Logic for GIF mappings and static assets
â”œâ”€â”€ create_dataset.py   # Script to convert raw images into landmark CSV
â”œâ”€â”€ hand_landmarks.csv  # Processed dataset of 21-point hand coordinates
â”œâ”€â”€ hand_tracker.py     # Real-time MediaPipe detection & stability logic
â”œâ”€â”€ label_encoder.pkl   # Decodes model predictions back to letters (0->A)
â”œâ”€â”€ logo.png            # Local logo file
â”œâ”€â”€ requirements.txt    # List of dependencies
â”œâ”€â”€ sign_language_model.h5 # The trained Custom Neural Network
â”œâ”€â”€ styles.css          # Custom CSS for "Aurora" theme and UI
â”œâ”€â”€ train_model.py      # Script used to train the Neural Network
â”œâ”€â”€ words_alpha.txt     # English dictionary for Predictive Text
â””â”€â”€ README.md           # Project Documentation

```

---

## ğŸ› ï¸ Prerequisites

![Python 3.10.0](https://img.shields.io/badge/python-3.10.0-3776AB?logo=python&logoColor=white)

> [!IMPORTANT]
> This project is **strictly dependent on Python 3.10.0** due to specific library compatibility requirements for **MediaPipe** and **TensorFlow**. 

Before running the project, please ensure you have the correct version installed:


1. **Verify your Python version:**
```bash
python --version
# Output must be Python 3.10.0

```



---

## âš™ï¸ Installation & Setup

1. **Clone the Repository**
```bash
git clone [https://github.com/yourusername/SignLingo.git](https://github.com/yourusername/SignLingo.git)
cd SignLingo

```


2. **Install Dependencies**
```bash
pip install -r requirements.txt

```


*Required: `streamlit`, `streamlit-webrtc`, `mediapipe`, `tensorflow`, `opencv-python`, `google-generativeai`, `SpeechRecognition`, `gTTS`.*
3. **Setup Assets**
* Ensure `sign_language_model.h5` and `label_encoder.pkl` are in the root directory.
* Ensure the `letters/` folder contains images named `a.jpg`, `b.jpg`, etc.
* Ensure `logo.png` is present.


4. **Run the App**
```bash
streamlit run app.py

```



---

## âš ï¸ Usage & Limitations

### âœ‹ Left-Hand Only Detection

> [!WARNING]
> **The current AI model has been trained exclusively on left-hand data.**
>
> * âœ… **DO:** Use your **LEFT HAND** to perform sign language gestures.
> * âŒ **DON'T:** Use your right hand; the model will may recognize the gestures accurately.

### ğŸ’¡ Best Practices

* Ensure your hand is clearly visible and well-lit.
* Keep your hand within the camera frame at all times.
* Avoid cluttered backgrounds.

---

## ğŸ§  How It Works

1. **Landmark Extraction:** Extracts 21 3D coordinates (x, y, z) using MediaPipe.
2. **Normalization:** Coordinates are normalized relative to the wrist.
3. **Classification:** Data is fed into a Feed-Forward Neural Network.
4. **Verification:** A "Stability Buffer" checks the last 15 frames for consistency.

---

## â“ Troubleshooting

**Camera not working?**

1. **Check Permissions:** Ensure your browser has allowed camera access.
2. **Network Firewalls:** If on a College/Office WiFi, try a **Mobile Hotspot**.
3. **Browser:** Use Google Chrome or Edge.

---

## ğŸ“œ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

* **Kaggle:** For the comprehensive ASL Alphabet Dataset.
* **Google:** For MediaPipe.
* **Streamlit:** For the web framework.

---

## ğŸ‘¨â€ğŸ’» Team

* **Varsha Suresh** - *Second Year Engineering CSE-(AIML), GSSS Institute of Engineering and Technology for Women, Mysore*
* **Tejas D.J** - *First Year Engineering AIML, BMS College of Engineering (BMSCE), Bengaluru*
