# ğŸ¤Ÿ SignLingo: AI-Powered Bidirectional Sign Language Translator

**SignLingo** is a real-time accessibility tool designed to bridge the communication gap between the Deaf/Hard-of-Hearing community and the hearing world. Built with advanced Computer Vision and Generative AI, it provides seamless **Sign-to-Text** and **Text-to-Sign** translation.

---

## ğŸš€ Key Features

### 1. ğŸ“· Real-Time Sign to Text

* **Custom AI Model:** Uses a custom-trained TensorFlow/Keras Neural Network (trained on the Kaggle ASL Alphabet dataset) to recognize hand gestures with 95%+ accuracy.
* **Smart Stabilization:** Implements a `Deque` sliding window algorithm to remove "flickering" and ensure smooth letter detection.
* **Predictive Text:** Auto-complete engine suggests words as you spell (e.g., spelling "P-Y-T" suggests "PYTHON"), significantly speeding up communication.
* **AI Grammar Correction:** Raw sign outputs (e.g., "ME GO STORE") are converted into fluent English sentences using **Google Gemini 1.5 Flash**.

### 2. ğŸ¤– Text to Sign (The "Infinite Dictionary")

* **Visual Translation:** Converts text input into dynamic Sign Language GIFs.
* **Fingerspelling Engine:** If a word (e.g., "ChatGPT") is not in the GIF database, the system automatically falls back to **Fingerspelling** the word letter-by-letter using a visual asset library.
* **Voice Input:** Integrated Speech-to-Text allows hearing users to speak into the microphone to generate signs instantly.

### 3. ğŸ® Gamified Learning Mode

* **Interactive Tutor:** A "Duolingo-style" module that challenges users to mimic specific ASL signs.
* **Real-time Feedback:** Uses Computer Vision to score the user's accuracy instantly.

---

## ğŸ› ï¸ Tech Stack

* **Frontend:** Streamlit (Python)
* **Computer Vision:** MediaPipe (Hand Landmarks), OpenCV
* **AI/ML:** TensorFlow (Keras), Scikit-Learn
* **Generative AI:** Google Gemini API (Grammar Correction)
* **Speech:** Google SpeechRecognition, gTTS (Google Text-to-Speech)
* **Hardware Acceleration:** Optimized for NVIDIA RTX GPUs (tested on RTX 3050).

---

## ğŸ“‚ Project Structure

```bash
SignLingo/
â”œâ”€â”€ letters&numbers/    # Database of images for the Fingerspelling Engine
â”œâ”€â”€ venv/               # Python Virtual Environment
â”œâ”€â”€ app.py              # Main application entry point (Streamlit)
â”œâ”€â”€ assets.py           # Logic for GIF mappings and static assets
â”œâ”€â”€ create_dataset.py   # Script to convert raw images into landmark CSV
â”œâ”€â”€ gemini_ai.py        # AI module for Grammar Correction & TTS
â”œâ”€â”€ hand_landmarks.csv  # Processed dataset of 21-point hand coordinates
â”œâ”€â”€ hand_tracker.py     # Real-time MediaPipe detection & stability logic
â”œâ”€â”€ label_encoder.pkl   # Decodes model predictions back to letters (0->A)
â”œâ”€â”€ logo.png            # Local logo file
â”œâ”€â”€ requirements.txt    # List of dependencies
â”œâ”€â”€ sign_language_model.h5 # The trained Custom Neural Network
â”œâ”€â”€ styles.css          # Custom CSS for "Aurora" theme and UI
â”œâ”€â”€ train_model.py      # Script used to train the Neural Network
â”œâ”€â”€ words_alpha.txt     # English dictionary for Predictive Text
â””â”€â”€ README.md           # Project Documentation


```



## ğŸ› ï¸ Prerequisites

This project is strictly dependent on **Python 3.10.0** due to specific library compatibility (MediaPipe/TensorFlow).

![Python Version](https://img.shields.io/badge/python-3.10.0-blue)

Before running the project, please ensure you have the correct version installed:

1. **Verify your Python version:**
   ```bash
   python --version
   # Output must be Python 3.10.0

---


## âš™ï¸ Installation & Setup

1. **Clone the Repository**
```bash
git clone https://github.com/yourusername/SignLingo.git
cd SignLingo

```


2. **Install Dependencies**
```bash
pip install -r requirements.txt

```


*Required libraries: `streamlit`, `streamlit-webrtc`, `mediapipe`, `tensorflow`, `opencv-python`, `google-generativeai`, `SpeechRecognition`, `gTTS`.*
3. **Setup Assets**
* Ensure `sign_language_model.h5` and `label_encoder.pkl` are in the root directory.
* Ensure the `letters/` folder contains images named `a.jpg`, `b.jpg`, etc., for the fingerspelling engine.
* Ensure `logo.png` is present.


4. **Run the App**
```bash
streamlit run app.py

---

## ğŸ”‘ Configuration & API Key

To fully utilize the **AI Grammar Correction** (Sign-to-Text) and **Voice Input** features, the app requires a Google Gemini API Key.

> **ğŸ‘¨â€âš–ï¸ For Judges:**
> A temporary API key has been provided below for evaluation purposes. Please copy and paste this into the **sidebar** of the application when prompted.

> **API Key:** `AIzaSyDAUHQgv74CG-svuUweXBeYMqKTjKM3FuU`
> *(Note: This key is restricted and will be revoked after the hackathon evaluation period.)*

---


## âš ï¸ Usage & Limitations

### âœ‹ Left-Hand Only Detection
**Important:** The current AI model has been trained exclusively on **left-hand** data.

* âœ… **DO:** Use your **LEFT HAND** to perform sign language gestures.
* âŒ **DON'T:** Use your right hand; the model will not recognize the gestures accurately.

### ğŸ’¡ Best Practices
For the most accurate detection:
* Ensure your hand is clearly visible and well-lit.
* Keep your hand within the camera frame at all times.
* Avoid cluttered backgrounds if possible.

---


## ğŸ§  How It Works

1. **Landmark Extraction:** The app uses MediaPipe to extract 21 3D coordinates (x, y, z) from the user's hand in real-time.
2. **Normalization:** These coordinates are normalized relative to the wrist to ensure the system works regardless of hand position or camera distance.
3. **Classification:** The normalized data is fed into a Feed-Forward Neural Network which classifies the gesture into one of 26 ASL alphabets.
4. **Verification:** A "Stability Buffer" checks the last 15 frames. If the prediction is consistent, the character is confirmed and added to the sentence.

---

## â“ Troubleshooting

**Camera not working / "Connection taking too long"?**

1. **Check Permissions:** Ensure your browser has allowed camera access.
2. **Network Firewalls:** If you are on a College/Office WiFi, the connection might be blocked. Try switching to a **Mobile Hotspot**.
3. **Browser:** Try using Google Chrome or Edge for best compatibility.

---

## ğŸ”® Future Scope

* **Dynamic Gesture Recognition:** Expanding the model to recognize moving gestures (like "Thank You" or "J") using LSTM (Long Short-Term Memory) networks.
* **Mobile App:** Porting the logic to Flutter/React Native for mobile accessibility.
* **Regional Languages:** Adding support for ISL (Indian Sign Language).

---


## ğŸ“œ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

* **Kaggle:** For the ASL Alphabet Dataset.
* **Google:** For the Gemini API and MediaPipe solutions.
* **Streamlit:** For the amazing web framework.


## ğŸ‘¨â€ğŸ’» Team

* **Varsha.Suresh**
* *Second Year Engineering CSE-(AIML), GSSS*
* **Tejas.D.J**
* *First Year Engineering (AIML), BMSCE*
